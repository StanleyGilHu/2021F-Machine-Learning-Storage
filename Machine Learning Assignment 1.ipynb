{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: What is Supervised Learning, unsupervised learning and difficulties involved in unsupervised learning and name a few supervised, unsupervised algorithms ? \n",
    "\n",
    "Supervised learning is learning where the datasets are labelled for the convenience of the machine learning algorithem so that they can diffrentiate between a victory condition and a non victory condition. In unsupervised learning, an algorithm is given a dataset and left to form its own conclusions from the data given. Issues with unsupervised learning include the fact that unsupervised learning could lead to the algorithm focusing on the wrong details.\n",
    "\n",
    "Examples of supervised learning include linear regression models. Examples of unsupervised learning include things like k means classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: What are parametric and non-parametric methods?. Explain pro’s and con’s of each.\n",
    "\n",
    "Parametric methods assume that there are underying rules or restrictions on the given sample populaltion for the algorithm, often weighing the algorithm to account for this, while non parametric models do not assume anything about the data, merely processing it as is. Parametric models require more labelling and consideration of data grouping, while non parametric models are less statistically significant and are harder to interpret, as they lack labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Explain about standard error , Co-variance and co-relation. When each one of them is\n",
    "used?\n",
    "\n",
    "Standard error is used to measure how members of a population are naturally distributed from a mean, such as sub populations used for sampling.\n",
    "\n",
    "Co variance is used to measure the linear relationship between two variables, and whether their variability have any correlation to each other. If one value rising causes the other to rise, and vice versa, the co variance is positive, approaching 1. If one value falls while the other rises, and vice versa, the co variance would be negative, approaching negative 1. If the two variables do not have any defining relationship with each other, the co variance would be around 0. This would be used \n",
    "\n",
    "Co relation is a function of Co variance, including both direction and strength of the relation, while co variance is more restricted to direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Explain how hypothesis testing is used in determining the linear relationship between the feature and target variable in linear regression? \n",
    "\n",
    "Hypothesis testing is calculating the sample variables to determine whether the null hypothesis or the experimental hypothesis is met. For hypothesis testing to work, the experimental observational data is compared against hypothetical predicted data, and any discrepancies are calculated to see how far away the results are from each other. If the probability of an outcome occuring is less than 5%, then the results will be considered statistically significant, and that there is a relationship between the feature/independant variable and the target/dependant variable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Please write down the equation of multi linear regression and explain each term present\n",
    "in it?\n",
    "\n",
    "y_i = beta0 + beta_1X_i1 + ...beta_px__ip + epsilon1 = X^T + Epsilon\n",
    "\n",
    "y_i represents y as the function of i, a dependant variable whose position changes based on the other variables provided, where y is the current position when all variables of i have been assigned a value. \n",
    "\n",
    "i represents a series of independant variables, each of which has their own trajectory and influence on the position of y as it changes. Beta is the coefficient constant of each variable, showing their impact on y.\n",
    "\n",
    "Epsilon is the y intercept, where the value of y is when every value of i is 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: What is a PCA? When to use PCA ? How does a PCA work? At least 5 sentences \n",
    "\n",
    "\n",
    "Principal Component Analysis is a means to sort and organize data by organizing data so that the line of best fit is perpendicular to the last previous line of best fit, where the principal components are two dimensional points representing the data to be analyzed. If a dataset is larger than two dimensions, it may be reduced to two dimension for PCA to work, while still retaining a large amount of variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: What are different pre-processing steps you can apply to the features before you can compute the PCA? Does every pre-processing method will yield same result or answer at the end of each method?\n",
    "\n",
    "Pre processing is the series of steps taken before examining the data to ensure that accurate and reliable results will be given. This may include dropping non numberical values such as arbitrary labels, rerepresenting some values as numbers, dropping data points which lack necessary values, and in some cases standardizing information to a new scale, such as from 0 to 1. Preprocessing methods often affect different parts of the data, so they will not always result in the same outcome if done seperately. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: What is clustering? Explain how K-Means Clustering Algorithm works?\n",
    "\n",
    "Clustering is forming data into discrete groups based on variable or relations between variables to better sort, organize, and associate the results. K-means clustering algorithm works by finding the mean location of a point or group of points, and linking the two closest means at any time in a population into a sub group, eventually creating larger and larger sub groups to encompass the entire population and determine relative distance between points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: What are the Advantages and disadvantages of Clustering ALgorithms discussed in our\n",
    "class (K-Means,Hierchal)?\n",
    "\n",
    "K-means is better in evaluating data that can be easily and accurately mathematically and graphically represented, such as data points on a xy or xyz axis. It can also be run several times to increase accuracy. However, it is often less accurate on data that have a lot of seperate variables, which can be hard to fit on a map, or variables which cannot be represented or seperated via distance. K-means is less efficient at detecting outliers. Also, while k-means can be run several times to get a more correct answer, the starting bias and locations can still affect the results, possibly disrupting the accuracy.\n",
    "\n",
    "Hierarchical clustering is better on data which may have a lot of variables that cannot be simplified away, and which be qualitative in nature rather than numerical.  However, hierachical clustering does not scale well when the number of data points themselves increases. It also has difficulty accurately representing the distances between each cluster or point. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Which Clustering Algorithm is better K-Means or hierarchical Clustering? Explain\n",
    "with a proper example which is better algorithm in each scenario?\n",
    "\n",
    "K-means is better in evaluating data that can be easily graphically represented and accurately mathematically processed, such as data points on a xy or xyz axis. For examples, length of a book by the number of pages compared to how many books were previously published in the series, or age of content consumers graphed against the amount of content they watch.\n",
    "\n",
    "Hierarchical clustering is better on groups of data with large amounts qualitative variables, and where general grouping is more important than numberically accurate grouping for the purposes of representation. An example of this might be in evolutionary clades, dividing phylums and families by the presence of their phenotypic traits, or in a dating app, sorting clients via their personal hobbies, occupations, and details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11:Please Perform Principal Component Analysis, Hierarchical and K-Means Clustering on the Give dataset Below. [50 Points]\n",
    "\n",
    "10 Points for Data Preprocessing.\n",
    "10 Points for PCA Algorithm along with plots and Results Explanation.\n",
    "10 Points for K-Means Algorithm with plots and Results Explanation.\n",
    "10 Points for Hierarchical Algorithm with plots and Results Explanation.\n",
    "10 Points for Comparing the results between PCA, Hierarchical Algorithm and K-Means and whats your inference from your outputs of the algorithms. Can you mention which algorithm works best for clustering on given dataset according to your plots?\n",
    "Hints:\n",
    "As per the data pre-processing step convert all the variables in the dataset into Numerical values as the algorithms only work with Numerical values. Check whether you need to standardization or normalization of data. Then Apply three algorithms one\n",
    "after the other then plot the output clusters\n",
    "Compare the output clusters in all the steps.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File kmeans_dataset.csv does not exist: 'kmeans_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1751c5bcdf26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Data preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmy_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kmeans_dataset.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmy_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Country or region\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File kmeans_dataset.csv does not exist: 'kmeans_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "#Data preprocessing\n",
    "my_df = pd.read_csv('kmeans_dataset.csv', skiprows=1)\n",
    "\n",
    "my_df = my_df.drop([\"Country or region\"],axis=1) \n",
    "#drops the country name, which cannot be nuemrically represented.\n",
    "normalized_df=(my_df-my_df.min())/(my_df.max()-my_df.min())\n",
    "print(nomalized_df)\n",
    "# x = StandardScaler().fit_transform(x)\n",
    "# above can also be used to standardize.\n",
    "\n",
    "#PCA\n",
    "\n",
    "pca = PCA(2)\n",
    "df = pca.fit_transform(my_df)\n",
    " \n",
    "df.shape\n",
    "\n",
    "\n",
    "principalDf = pd.DataFrame(data = df, columns = ['principal component 1', 'principal component 2'])\n",
    "finalDf = pd.concat([principalDf, df[['target']]], axis = 1)\n",
    "\n",
    "\n",
    "#plotting for the pca plot?\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "\n",
    "\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "#k-means\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters= 10)\n",
    " \n",
    "#predict the labels of clusters.\n",
    "label = kmeans.fit_predict(my_df)\n",
    " \n",
    "print(label)\n",
    "\n",
    "filtered_label0 = df[label == 0]\n",
    "\n",
    "plt.scatter(filtered_label0[:,0] , filtered_label0[:,1])\n",
    "plt.show()\n",
    "\n",
    "#Getting unique labels\n",
    "u_labels = np.unique(label)\n",
    " \n",
    "#plotting the results:\n",
    "for i in u_labels:\n",
    "    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#hiearchical clustering\n",
    "\n",
    "\n",
    "\n",
    "labels = range(1, 11)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "plt.scatter(X[:,0],X[:,1], label='True Position')\n",
    "\n",
    "for label, x, y in zip(labels, X[:, 0], X[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-3, 3),\n",
    "        textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 12: Answer the following? 50 pts]\n",
    "a) Write and explain about the Linear Regression and it’s equation 3 pts]\n",
    "b) Explain in detail about the loss function of linear regression, R2, Adjusted R2 used in the Linear Regression and what is the need for Adjusted R2? 12 pts]\n",
    "c) Plot X vs Y in Scatter plot from data in Table 1 and comment on the relation of X vs Y using Covariance,Corelation. Please comment on Covariance and corelation values 5 Pts\n",
    "d) Perform Linear regression on the following data using Python? and print β0, β1 values in equation y= β0+ β1*x. Please write down what is your understanding from those values. 10 Pts\n",
    "\n",
    "\n",
    "\n",
    "a) The equation for linear regression is Y_i=f(X_i, \\beta)+e_i, where Y_i is the dependant fvariable, f is the function, x is the independant variable, beta represents unknown parameters, and e_i represents error terms\n",
    "\n",
    "b) Loss functions are used to estimate how inaccurate a model is, and used to provide feedback to the machine learning network so that it can continue to improve its estimations as it continues to evaluate the data. The R^2 function is used to model how far away the linear regression line is from the sample data in order to measure and judge the correctness. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: 341.25000000000006\n",
      "slope: [26.25]\n",
      "y =  341.25000000000006  +  [26.25] x\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD0CAYAAABtjRZ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWo0lEQVR4nO3df0yV5/3/8dcROICAY8a6VB0WlqJEQ0YwthvVlkZH1m+yEJUzOR3NxK8bzP0hzZxiJm45mUpMjd1i/JFo9tnB9oQJo/8sJYu6uaoh5oQVdQeTUl1WaBkfqxnn7HBEej5/dFKd4oHDOZxznfN8/GWum/s+7zehr5xe93XflyUYDAYFADDSrFgXAAAIHyEOAAYjxAHAYIQ4ABiMEAcAgxHiAGCw1Jn+QLfbPdMfCQDGKy0tfez4jIe4NHExoXg8HhUVFUW4mthIlF4SpQ+JXuJRovQhTa+XJ335ZToFAAxGiAOAwQhxADAYIQ4ABiPEAcBgMVmdAgDJoqO7Xwc6r2vgjl8Lcj/W9oolqixZGLHrE+IAECUd3f1qbL8i/+iYJKn/jl+N7VckKWJBznQKAETJgc7r4wF+n390TAc6r0fsMwhxAIiSgTv+KY2HgxAHgChZkJs5pfFwEOIAECXbK5YoMy3lobHMtBRtr1gSsc/gxiYARMn9m5dfrE7JZHUKAJiksmShKksWRu1lXkynAIDBCHEAMBghDgAGI8QBwGCEOAAYjBAHAIMR4gBgMEIcAAxGiAOAwQhxADAYIQ4ABiPEAcBgk3oB1rFjx3T27FmNjo6qurpaK1eu1M6dO2WxWPTss89qz549mjVrllpbW+VyuZSamqr6+nqVl5dHu34ASGohv4l3dXWpu7tbb7/9tpxOpz755BPt27dP27Zt01tvvaVgMKgzZ85oaGhITqdTLpdLJ06c0MGDB3X37t2Z6AEAklbIEH/vvfdUWFiorVu3qq6uTi+99JKuXbumlStXSpJWr16tixcvqqenRyUlJbJarcrJyVFeXp56e3uj3gAAJLOQ0ym3b9/WwMCAjh49qo8++kj19fUKBoOyWCySpKysLA0PD8vr9SonJ2f8vKysLHm93uhVDgAIHeK5ubkqKCiQ1WpVQUGB0tPT9cknn4wf9/l8mjNnjrKzs+Xz+R4afzDUH+TxeMIqdmRkJOxz402i9JIofUj0Eo8SpQ8per2EDPHS0lL99re/1aZNm/TPf/5Tfr9f3/jGN9TV1aXnnntO58+f1/PPP6/i4mIdOnRIgUBAd+/eVV9fnwoLCx97zXB3t4jWzhixkCi9JEofEr3Eo0TpQ5peL263e8JjIUO8vLxcly9f1oYNGxQMBtXU1KRFixZp9+7dOnjwoAoKClRRUaGUlBTV1NTIbrcrGAyqoaFB6enpYRUMAJicSS0x/OlPf/rIWEtLyyNjNptNNptt+lUBACaFjZIBIIo6uvsf2O3+Y3a7BwBTdHT3q7H9ivyjY5Kk/jt+NbZfkaSIBTmP3QNAlBzovD4e4Pf5R8d0oPN6xD6DEAeAKBm445/SeDgIcQCIkgW5mVMaDwchDgBRsr1iiTLTUh4ay0xL0faKJRH7DG5sIulFe/UAktf9v6Mv/r4yWZ0CRNJMrB5AcqssWajKkoVRe/qU6RQktZlYPQBEEyGOpDYTqweAaCLEkdRmYvUAEE2EOJLaTKweAKKJG5tIajOxegCIJkIcSS/aqweAaCLEkfRYJw6TEeJIaqwTh+m4sYmkxjpxmI4QR1JjnThMR4gjqbFOHKYjxJHUWCcO03FjE0mNdeKINvbYBKKMdeKIFvbYBACDsccmABiMPTYBwGDssQkABmOPTQAwWNzssVlZWamcnBxJ0qJFi/S9731PdXV1euaZZyRJ1dXVeuWVV9Ta2iqXy6XU1FTV19ervLw8YoUCgImivfopZIgHAgFJktPpHB/73e9+p02bNqm2tnZ8bGhoSE6nU21tbQoEArLb7SorK5PVao140QCAz4UM8d7eXvn9ftXW1urevXt6/fXXdfXqVd24cUNnzpzR4sWLtWvXLvX09KikpERWq1VWq1V5eXnq7e1VcXHxTPQBAEkpZIhnZGRo8+bNqqqq0s2bN7Vlyxb94Ac/UFVVlZYvX64jR47o8OHDWrp06fiUiyRlZWXJ6/U+9poejyesYkdGRsI+N94kSi+J0odEL/EoUfqQotdLyBDPz8/X4sWLZbFYlJ+fr9zcXK1atUpPP/20JGnt2rVyOBxasWKFfD7f+Hk+n++hUH9QuPNCifREXaL0kih9SPQSjxKlD2l6vbjd7gmPhVxiePr0ae3fv1+SNDg4KK/Xq61bt6qnp0eSdOnSJS1btkzFxcVyu90KBAIaHh5WX1+fCgsLwyoYADA5Ib+Jb9iwQY2NjaqurpbFYtHevXuVnp4uh8OhtLQ0zZs3Tw6HQ9nZ2aqpqZHdblcwGFRDQ4PS09NnogcASFohQ9xqteqNN954ZNzlcj0yZrPZZLPZIlMZACAkntgEAIMR4gBgMEIcAAxGiAOAwQhxADAYIQ4ABiPEAcBghDgAGIwQBwCDsbMPkl5Hd/8DO698HPGdV4BoIsSR1Dq6+9XYfkX+0TFJUv8dvxrbr0gSQQ4jMJ2CpHag8/p4gN/nHx3Tgc7rMaoImBpCHElt4I5/SuNAvCHEkdQW5GZOaRyIN4Q4ktr2iiXKTEt5aCwzLUXbK5bEqCJgarixiaR2/+blF6tTMlmdAqMQ4kh6lSULVVmyMKH2c0TyYDoFAAxGiAOAwQhxADAYIQ4ABiPEAcBghDgAGIwQBwCDEeIAYDBCHAAMRogDgMEm9dh9ZWWlcnJyJEmLFi1SXV2ddu7cKYvFomeffVZ79uzRrFmz1NraKpfLpdTUVNXX16u8vDyqxQNAsgsZ4oFAQJLkdDrHx+rq6rRt2zY999xzampq0pkzZ/T1r39dTqdTbW1tCgQCstvtKisrk9VqjV71AJDkQoZ4b2+v/H6/amtrde/ePb3++uu6du2aVq5cKUlavXq1Lly4oFmzZqmkpERWq1VWq1V5eXnq7e1VcXFx1JsAgGQVMsQzMjK0efNmVVVV6ebNm9qyZYuCwaAsFoskKSsrS8PDw/J6veNTLvfHvV7vY6/p8XjCKnZkZCTsc+NNovSSKH1I9BKPEqUPKXq9hAzx/Px8LV68WBaLRfn5+crNzdW1a9fGj/t8Ps2ZM0fZ2dny+XwPjT8Y6g8K93WfifSq0ETpJVH6kOglHiVKH9L0enG73RMeC7k65fTp09q/f78kaXBwUF6vV2VlZerq6pIknT9/XitWrFBxcbHcbrcCgYCGh4fV19enwsLCsAoGAExOyG/iGzZsUGNjo6qrq2WxWLR37159+ctf1u7du3Xw4EEVFBSooqJCKSkpqqmpkd1uVzAYVENDg9LT02eiBwBIWiFD3Gq16o033nhkvKWl5ZExm80mm80WmcoAACHxsA8AGIwQBwCDEeIAYDBCHAAMRogDgMEIcQAwGCEOAAYjxAHAYIQ4ABiMEAcAgxHiAGAwQhwADDapPTYBAOHp6O7Xgc7rGrjj14Lcj7W9YokqSxZG7PqEOABESUd3vxrbr8g/OiZJ6r/jV2P7FUmKWJAznQIAUXKg8/p4gN/nHx3Tgc7rEfsMQhwAomTgjn9K4+EgxAEgShbkZk5pPByEOABEyfaKJcpMS3loLDMtRdsrlkTsM7ixCQBRcv/m5RerUzJZnQIAJqksWajKkoXyeDwqKiqK+PWZTgEAgxHiAGAwQhwADEaIA4DBCHEAMBghDgAGI8QBwGCTCvFbt27pxRdfVF9fn65du6ZVq1appqZGNTU1+sMf/iBJam1t1bp162Sz2XTu3LmoFg0A+FzIh31GR0fV1NSkjIwMSdLf/vY3bdq0SbW1teM/MzQ0JKfTqba2NgUCAdntdpWVlclqtUavcgBA6G/izc3N2rhxo+bPny9Junr1qv70pz/p1Vdf1a5du+T1etXT06OSkhJZrVbl5OQoLy9Pvb29US8eAJLdE7+Jt7e3a+7cuVq1apWOHz8uSSouLlZVVZWWL1+uI0eO6PDhw1q6dKlycnLGz8vKypLX653wuh6PJ6xiR0ZGwj433iRKL4nSh0Qv8ShR+pCi18sTQ7ytrU0Wi0WXLl2Sx+PRjh07dOTIET311FOSpLVr18rhcGjFihXy+Xzj5/l8vodC/b+F+/6AaL17IBYSpZdE6UOil3iUKH1I0+vF7XZPeOyJ0ymnTp1SS0uLnE6nioqK1NzcrB/96Efq6emRJF26dEnLli1TcXGx3G63AoGAhoeH1dfXp8LCwrCKfZyO7n6V7T+rV/7nQ5XtP6uO7v6IXRsATDbltxj+/Oc/l8PhUFpamubNmyeHw6Hs7GzV1NTIbrcrGAyqoaFB6enpESlwJvaoAwBTTTrEnU7n+L9dLtcjx202m2w2W2SqesCT9qgjxAEku7h/2Gcm9qgDAFPFfYjPxB51AGCquA/xmdijDgBMFffbs83EHnVIbh3d/Q/8fX3M3xeMEvchLkV/jzokL1Y/wXRxP50CRNOTVj8BJiDEkdRY/QTTEeJIaqx+gukIcSQ1Vj/BdEbc2ASihdVPMB0hjqTH6ieYjOkUADCYEd/EeRgDAB4v7kOchzEAYGJxP53CwxgAMLG4D3EexgCAicV9iPMwBgBMLO5DnIcxAGBicR/ilSULtb50oVIsFklSisWi9aULuakJADIgxDu6+9Xm7tdYMChJGgsG1ebuZ8d7AJABIc7qFACYWNyHOKtTAGBicR/irE4BgInFfYizOgUAJhb3j93zqlAAmFjch7jEq0IBYCJxP50CAJjYpEL81q1bevHFF9XX16e///3vqq6ult1u1549e/TZZ59JklpbW7Vu3TrZbDadO3cuqkUDAD4XMsRHR0fV1NSkjIwMSdK+ffu0bds2vfXWWwoGgzpz5oyGhobkdDrlcrl04sQJHTx4UHfv3o168QCQ7EKGeHNzszZu3Kj58+dLkq5du6aVK1dKklavXq2LFy+qp6dHJSUlslqtysnJUV5ennp7e6NbOQDgyTc229vbNXfuXK1atUrHjx+XJAWDQVn+8x6TrKwsDQ8Py+v1KicnZ/y8rKwseb3eCa/r8XjCKnZkZCTsc+NNovSSKH1I9BKPEqUPKXq9PDHE29raZLFYdOnSJXk8Hu3YsUOffvrp+HGfz6c5c+YoOztbPp/vofEHQ/2/hbvCJJFWpyRKL4nSh0Qv8ShR+pCm14vb7Z7w2BOnU06dOqWWlhY5nU4VFRWpublZq1evVldXlyTp/PnzWrFihYqLi+V2uxUIBDQ8PKy+vj4VFhaGVSwAYPKmvE58x44d2r17tw4ePKiCggJVVFQoJSVFNTU1stvtCgaDamhoUHp6ejTqBQA8YNIh7nQ6x//d0tLyyHGbzSabzRaZqgAAk8LDPgBgMEIcAAxGiAOAwQhxADAYIQ4ABiPEAcBghDgAGIwQBwCDEeIAYDBCHAAMRogDgMEIcQAwGCEOAAYjxAHAYIQ4ABiMEAcAgxHiAGAwQhwADEaIA4DBCHEAMBghDgAGI8QBwGCEOAAYjBAHAIMR4gBgMEIcAAyWGuoHxsbG9LOf/Uw3btxQSkqK9u3bp+HhYdXV1emZZ56RJFVXV+uVV15Ra2urXC6XUlNTVV9fr/Ly8mjXDwBJLWSInzt3TpLkcrnU1dWlffv26eWXX9amTZtUW1s7/nNDQ0NyOp1qa2tTIBCQ3W5XWVmZrFZr9KoHgCQXMsTXrFmjl156SZI0MDCgefPm6erVq7px44bOnDmjxYsXa9euXerp6VFJSYmsVqusVqvy8vLU29ur4uLiaPcAAEkrZIhLUmpqqnbs2KE//vGP+tWvfqXBwUFVVVVp+fLlOnLkiA4fPqylS5cqJydn/JysrCx5vd6IFNnR3a8Dndc1cMevBbkfa3vFElWWLIzItQHAZJMKcUlqbm7WT37yE9lsNrlcLn3lK1+RJK1du1YOh0MrVqyQz+cb/3mfz/dQqD/I4/FMusCzHw7rVxf/V4GxoCSp/45fO06/r/6Bfr1c8Pjrm2BkZGRKv4d4lSh9SPQSjxKlDyl6vYQM8Y6ODg0ODuqHP/yhMjMzZbFY9OMf/1i7d+9WcXGxLl26pGXLlqm4uFiHDh1SIBDQ3bt31dfXp8LCwsdes6ioaNIF/v93zo4H+H2BsaDeuuLV1v+3ctLXiTcej2dKv4d4lSh9SPQSjxKlD2l6vbjd7gmPhQzxb33rW2psbNSrr76qe/fuadeuXXr66aflcDiUlpamefPmyeFwKDs7WzU1NbLb7QoGg2poaFB6enpYBT9o4I5/SuMAkExChvjs2bP15ptvPjLucrkeGbPZbLLZbJGp7D8W5Gaq/zGBvSA3M6KfAwAmivuHfbZXLFFmWspDY5lpKdpesSRGFQFA/Jj0jc1Yub8K5YvVKZmsTgGA/4j7EJc+D/LKkoUJdZMDACIh7qdTAAATI8QBwGCEOAAYjBAHAIMR4gBgMEswGAyG/rHIedLjowCAxystLX3s+IyHOAAgcphOAQCDEeIAYLC4f2LzcXt85uXlxbqsabl165bWrVunkydP6mtf+1qsywlbZWXl+DvjFy1apH379sW4ovAdO3ZMZ8+e1ejoqKqrq1VVVRXrkqasvb1dv//97yVJgUBAHo9HFy5c0Jw5c2Jc2dSNjo5q586d6u/v16xZs+RwOIz9b+Xu3btqbGzUP/7xD2VnZ6upqWl8f+JIiPsQf9wen0eOHIlxVeEbHR1VU1OTMjIyYl3KtAQCAUmS0+mMcSXT19XVpe7ubr399tvy+/06efJkrEsKy7p167Ru3TpJ0i9+8QutX7/eyACXpD//+c+6d++eXC6XLly4oEOHDunXv/51rMsKS2trq2bPnq3W1lZ9+OGHcjgcOnHiRMSuH/fTKWvWrJHD4ZD0xR6fJmtubtbGjRs1f/78WJcyLb29vfL7/aqtrdVrr72mv/71r7EuKWzvvfeeCgsLtXXrVtXV1Y3vKWuqK1eu6IMPPtB3v/vdWJcStvz8fI2Njemzzz6T1+tVamrcf9+c0AcffKDVq1dLkgoKCtTX1xfR6xvxm/nvPT5N1d7errlz52rVqlU6fvx4rMuZloyMDG3evFlVVVW6efOmtmzZonfffdfI/9hu376tgYEBHT16VB999JHq6+v17rvvymKxxLq0sBw7dkxbt26NdRnTMnv2bPX39+vb3/62bt++raNHj8a6pLAVFRXp3LlzWrNmjd5//30NDg5qbGxMKSkpoU+ehLj/Jn5fc3OzOjs7tXv3bv373/+OdTlhaWtr08WLF1VTUyOPx6MdO3ZoaGgo1mWFJT8/X9/5zndksViUn5+v3NxcY3vJzc3VCy+8IKvVqoKCAqWnp+vTTz+NdVlh+de//qUPP/xQzz//fKxLmZbf/OY3euGFF9TZ2al33nlHO3fuHJ/CM8369euVnZ2t1157TefOndOyZcsiFuCSASHe0dGhY8eOSdL4Hp+R/AXMpFOnTqmlpUVOp1NFRUVqbm7WU089FeuywnL69Gnt379fkjQ4OCiv12tsL6WlpfrLX/6iYDCowcFB+f1+5ebmxrqssFy+fFnf/OY3Y13GtM2ZM2f8pvmXvvQl3bt3T2NjYzGuKjxXrlxRaWmpnE6n1qxZo69+9asRvX7c/7/v4/b4jMTenZieDRs2qLGxUdXV1bJYLNq7d6+RUymSVF5ersuXL2vDhg0KBoNqamoy9ovCjRs3tGjRoliXMW3f//73tWvXLtntdo2OjqqhoUGzZ8+OdVlhWbx4sd58802dPHlSOTk5+uUvfxnR6/PEJgAYLO6nUwAAEyPEAcBghDgAGIwQBwCDEeIAYDBCHAAMRogDgMEIcQAw2P8BPk3HCoKL5ooAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#c)\n",
    "\n",
    "#X= np.array([[6,526],[3,421],[6,581],[9,630],[3,412],[9,560],[6,434],[3,443],[9,590],[6,570],[3,346],[9,472]])\n",
    "#scatter(X,xvar,yvar)\n",
    "\n",
    "X = np.array([6, 3, 6, 9, 3, 9, 6, 3, 9, 6, 3, 9]).reshape((-1, 1))\n",
    "Y = np.array([526, 421, 581, 630, 412, 560, 434, 443, 590, 570, 346, 472])\n",
    "plt.scatter(X,Y)\n",
    "\n",
    "#d)\n",
    "model = LinearRegression().fit(X, Y)\n",
    "print('intercept:', model.intercept_)\n",
    "print('slope:', model.coef_)\n",
    "\n",
    "print('y = ',model.intercept_,' + ',model.coef_,'x')\n",
    "\n",
    "print('When x = 0, the value of y is 341.25. As X increases by one value, there is a rise of 26.25 in Y.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 13: What are different evaluation metrics available for predicting the performance of the Linear Regression? Evaluate all those methods on the given dataset in Table 1 and also please print out the accuracy, R2 , Adjusted R2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different evaluation metrics for Linear Regression include R2, adjusted R2, Mean Error, absolute error, among others. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.5715182600732074\n",
      "Accuracy? 57.151826007320736 %\n",
      "adjusted coefficient of determination: 0.5286700860805281\n"
     ]
    }
   ],
   "source": [
    "h = np.array([6,3, 6, 9, 3, 9, 6, 3, 9, 6, 3, 9]).reshape((-1, 1))\n",
    "i = np.array([526, 421, 581, 630, 412, 560, 434, 443, 590, 570, 346, 472])\n",
    "model = LinearRegression().fit(h, i)\n",
    "\n",
    "\n",
    "\n",
    "r_sq = model.score(h, i)\n",
    "print('coefficient of determination:', r_sq)\n",
    "\n",
    "#results = model.fit(h,i)\n",
    "#print('adjusted coefficient of determination:', results.rsquared_adj)\n",
    "\n",
    "#above lines not working.\n",
    "\n",
    "#Accuracy? \n",
    "regressor = LinearRegression()\n",
    "#regressor.fit(x_train,y_train)\n",
    "#r2_score = regressor.score(x_test,y_test)\n",
    "regressor.fit(h,i)\n",
    "r2_score = regressor.score(h,i)\n",
    "print('Accuracy?',r2_score*100,'%')\n",
    "\n",
    "\n",
    "#Adjusted r Squared\n",
    "\n",
    "print('adjusted coefficient of determination:', 1 - (1-model.score(h, i))*(len(i)-1)/(len(i)-h.shape[1]-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
